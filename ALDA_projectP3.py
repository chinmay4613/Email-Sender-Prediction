# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R1Dp4uRV3aBfb4dYp0ypHHWe63D4cr7F
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
import re
import io
from collections import Counter
from tqdm import tqdm
from time import time
import nltk
nltk.download('stopwords')
from tensorflow import keras
from keras.layers import Dense
from keras.models import Sequential, load_model

import tensorflow as tf
from tensorflow.keras import models, layers, optimizers, losses, metrics, utils, applications, callbacks
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


# reading the data
data = pd.read_csv('emails.csv')
data.head()   #print head

# For removing stopwords from the text (replacing it with '')
from nltk.corpus import stopwords
stopwords_eng = set(stopwords.words('english'))
def remove_stopwords(txt):
    for stopword in stopwords_eng:
        while stopword in txt:
            txt = txt.replace(stopword,'')
    return txt

# splitting Training, testing and validation data in the ratio 8:1:1
from sklearn.model_selection import train_test_split as tts
def data_set_split(X, y, split_sizes=[8,1,1], stratify=True):  #stratify to ensure balanced class distribution
    split_sizes = np.array(split_sizes)
    if stratify:
        train_X, test_X, train_y, test_y = tts(X, y, test_size=split_sizes[2]/split_sizes.sum(), stratify=y)
        train_X, val_X, train_y, val_y = tts(train_X, train_y, test_size=split_sizes[1]/(split_sizes[0]+split_sizes[1]), stratify=train_y)
    else:
        train_X, test_X, train_y, test_y = tts(X, y, test_size=split_sizes[2]/split_sizes.sum())
        train_X, val_X, train_y, val_y = tts(train_X, train_y, test_size=split_sizes[1]/(split_sizes[0]+split_sizes[1]))
    return train_X, val_X, test_X, train_y, val_y, test_y

print(data.message[0])

sender_pattern = re.compile('(?<=From:\s).*') # extract the sender of the message
xfile_pattern = re.compile('X-FileName:.*') #extract the file name
content_pattern = re.compile('[^\n].*\Z', re.S)  #extract the content of the mail. re.S flag used to match '.' to newline

xfile_end = xfile_pattern.search(data.message[0]).end()
content = content_pattern.search(data.message[0], pos=xfile_end)  # content after Xfilename is stored in content

print(data.message[0])
print(sender_pattern.search(data.message[0]).group())
print("Message:")
print(content.group())

data['Sender'] = data.message.apply(lambda x: sender_pattern.search(x).group()) #add a sender column and populate it using sender_pattern function
data['Content'] = data.message.apply(
    lambda x: content_pattern.search( x, pos = xfile_pattern.search(x).end()).group())   # add a Content column
data['Length'] = data.Content.apply(lambda x: len(x)) #add a Length column denoting the length of the message

print(data.isnull().any())  #checking if our columns have any null values (improper column formation)
data.head()

data.Sender.value_counts()[:15] #extract top 15 frequent email senders
import matplotlib.pyplot as plt
import seaborn as sns

# Extract top 15 frequent email senders
top_senders = data['Sender'].value_counts()[:15]

# Plot the data
plt.figure(figsize=(10, 6))
sns.barplot(x=top_senders.values, y=top_senders.index, palette='viridis')
plt.title('Top 15 Frequent Email Senders')
plt.xlabel('Count')
plt.ylabel('Sender')
plt.show()

import matplotlib.pyplot as plt

# Assuming 'Length' column is already created in your DataFrame
# If not, make sure to create it using: data['Length'] = data['Content'].apply(len)

# Define bin edges based on the desired range and bin size
bin_edges = list(range(0, 2001, 50))

# Create bins and plot histogram
plt.hist(data['Length'], bins=bin_edges, edgecolor='black', alpha=0.7)

# Set labels and title
plt.xlabel('Length')
plt.ylabel('Frequency')
plt.title('Histogram of Lengths with Bins of Size 50 (0-2000)')

# Show the plot
plt.show()

"""prepare model for classification"""

senders_list = data.Sender.value_counts().index[:15].tolist() # Find Top 15 senders
messages_per_class = 1024  #sample random 1024 messages of each sender

texts = [] # preprocessed content of the emails will be stored here
labels = [] # corresponding labels (sender indices) will be stored.
for i, sender in tqdm(enumerate(senders_list)):
    sender_texts = data.query(' `Sender` == @sender ').sample(frac=1)['Content'][:messages_per_class].apply(lambda x: remove_stopwords(x)).values.tolist() #extract the content of 1024 messages from each sender
    texts += sender_texts
    labels += (np.ones(shape=(messages_per_class,))*i).tolist()

max_words = 512   #512 Maximum number of words the model will learn. depends on the diversity of vocabulary
embed_dim = 128      #128 Number of embedding dimensions. to capture complex relationships between words
maxlen =  512        #The length of the message - The longer messages will be cropped to 512, shorter ones will be padded with zeros


tokenizer = Tokenizer(num_words=max_words) # seperate each word into tokens
tokenizer.fit_on_texts(texts)
word_index = tokenizer.word_index

seqs = tokenizer.texts_to_sequences(texts) # convert words into sequences of integers corresponding to 512 most freq words
seqs = pad_sequences(seqs, maxlen=maxlen) # pad sequence to make it 512 tokens long

labels = np.array(labels)

train_X, val_X, test_X, train_y, val_y, test_y = data_set_split(seqs, labels, stratify=True) # split sequences into training, validation and testing sets in 8:1:1

# ensure equal distribution of classes
for X, y in [train_X, train_y], [val_X, val_y], [test_X, test_y]:
    print(X.shape, y.shape)
    print(np.bincount(y.astype(np.int32)))

"""#**Build and train models.**

Naive Bayes (baseline)
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Use CountVectorizer to convert text data into a bag-of-words representation
vectorizer = CountVectorizer(max_features=max_words)
X = vectorizer.fit_transform(texts).toarray()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

# Initialize and train a Multinomial Naive Bayes classifier
baseline_model = MultinomialNB()
baseline_model.fit(X_train, y_train)

# Make predictions on the test set
predictions = baseline_model.predict(X_test)

# Evaluate the baseline model
accuracy = accuracy_score(y_test, predictions)
print(f"Baseline Model Accuracy: {accuracy:.2%}")

# Display additional classification metrics
print(classification_report(y_test, predictions, target_names=senders_list))

"""#LSTM

"""

import tensorflow as tf
from keras.layers import Bidirectional, LSTM, Dense, Embedding, Dropout

# Define the model architecture
modelLSTM = tf.keras.Sequential([
    # Embedding layer to convert words into vectors
    Embedding(input_dim=max_words, output_dim=embed_dim, input_length=maxlen),

    # Bidirectional LSTM layer to learn temporal dependencies in the text
    Bidirectional(LSTM(units=64, return_sequences=True)),
    Bidirectional(LSTM(units=32)),

    # Dropout layer to prevent overfitting
    Dropout(rate=0.2),

    # Dense layer to learn a mapping from the LSTM output to the class labels
    Dense(units=15, activation='softmax')  # Adjust the number of units to match the number of classes
])

# Compile the model
modelLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

from keras.utils import to_categorical

def learningRate_scheduler(epoch, lr):
    if epoch==8 or epoch==12:
        return lr/8
    else:
        return lr


callbacks_list = [
    callbacks.ModelCheckpoint('best_modelLSTM.h5', monitor='val_loss', save_best_only=True, save_freq='epoch'),
    callbacks.LearningRateScheduler(learningRate_scheduler)
]

modelLSTM.compile(
    optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics=['acc']
)

EPOCHS = 16

train_y_one_hot = to_categorical(train_y, num_classes=15)
val_y_one_hot = to_categorical(val_y, num_classes=15)

historyLSTM = modelLSTM.fit(
    train_X, train_y_one_hot,
    validation_data=(val_X, val_y_one_hot),
    epochs=EPOCHS, batch_size=64,

    shuffle=True,
    verbose=1,
    callbacks=callbacks_list
)


pd.DataFrame(historyLSTM.history).to_csv('historyLSTM.csv')
modelLSTM.save('last_modelLSTM.h5')

train_loss = historyLSTM.history['loss']
val_loss = historyLSTM.history['val_loss']

train_acc = historyLSTM.history['acc']
val_acc = historyLSTM.history['val_acc']

lr = historyLSTM.history['lr']

x = np.arange(1, EPOCHS+1)

#compares training and validation loss across all epochs
plt.plot(x, train_loss, 'b--', label='Training loss')
plt.plot(x, val_loss, 'r-', label='Validation loss')
plt.title('Training and Validation loss')
plt.legend()
plt.show()

#compares training and validation accuracy across all epochs
plt.plot(x, train_acc, 'b--', label='Training accuracy')
plt.plot(x, val_acc, 'r-', label='Validation accuracy')
plt.title('Training and Validation accuracy')
plt.legend()
plt.show()

#plots the learning rate showing change after 8th and 12th epochs
plt.plot(x, lr, 'g-', label='Learning Rate')
plt.yscale('log')
plt.title('Learning rate')
plt.legend()
plt.show()

modelLSTM.load_weights('last_modelLSTM.h5')
modelLSTM.compile(        #last model
    optimizer='rmsprop',
    loss='sparse_categorical_crossentropy',
    metrics=['acc']
)
print("\t\tLAST MODEL:")
print('Training set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelLSTM.evaluate(train_X, train_y, verbose=0)))
print('Validation set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelLSTM.evaluate(val_X, val_y, verbose=0)))
print('Testing set metrics:\t\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelLSTM.evaluate(test_X, test_y, verbose=0)))

best_modelLSTM = models.load_model('best_modelLSTM.h5')
best_modelLSTM.compile(
    optimizer='rmsprop',
    loss='sparse_categorical_crossentropy',
    metrics=['acc']
)
print("\n\t\tBEST MODEL:")
print('Training set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelLSTM.evaluate(train_X, train_y, verbose=0)))
print('Validation set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelLSTM.evaluate(val_X, val_y, verbose=0)))
print('Testing set metrics:\t\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelLSTM.evaluate(test_X, test_y, verbose=0)))

from sklearn.metrics import precision_score, recall_score, f1_score

# Assuming predictions are obtained using modelCNN.predict()

predictions = modelLSTM.predict(test_X)
predicted_labels = np.argmax(predictions, axis=1)

precision = precision_score(test_y, predicted_labels, average='weighted')
recall = recall_score(test_y, predicted_labels, average='weighted')
f1 = f1_score(test_y, predicted_labels, average='weighted')

print('Precision: %.4f' % precision)
print('Recall: %.4f' % recall)
print('F1 Score: %.4f' % f1)

"""#CNN"""

from tensorflow.keras import Sequential
modelCNN = Sequential()

# Embedding layer to convert words into vectors
modelCNN.add(layers.Embedding(input_dim=max_words, output_dim=embed_dim, input_length=maxlen))

# Convolutional layers to extract local patterns from the word vectors
modelCNN.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))
modelCNN.add(layers.MaxPooling1D(pool_size=2))
modelCNN.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))
modelCNN.add(layers.MaxPooling1D(pool_size=2))

# Flatten the output of the convolutional layers into a 1D vector
modelCNN.add(layers.Flatten())

# Fully connected layers to learn higher-level features from the flattened output
modelCNN.add(layers.Dense(units=64, activation='relu', kernel_regularizer='l2'))
modelCNN.add(layers.BatchNormalization())
modelCNN.add(layers.Dropout(rate=0.2))
modelCNN.add(layers.Dense(units=32, activation='relu', kernel_regularizer='l2'))
modelCNN.add(layers.BatchNormalization())
modelCNN.add(layers.Dropout(rate=0.1))

# Output layer with softmax activation for multi-class classification
modelCNN.add(layers.Dense(units=15, activation='softmax'))  # Adjust the number of units to match the number of classes

# Compile the modelCNN
modelCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

def learningRate_scheduler(epoch, lr):
    if epoch==8 or epoch==12:
        return lr/8
    else:
        return lr


callbacks_list = [
    callbacks.ModelCheckpoint('best_modelCNN.h5', monitor='val_loss', save_best_only=True, save_freq='epoch'),
    callbacks.LearningRateScheduler(learningRate_scheduler)
]

modelCNN.compile(
    optimizer='rmsprop',
    loss='sparse_categorical_crossentropy',
    metrics=['acc']
)

EPOCHS = 16

historyCNN = modelCNN.fit(
    train_X, train_y,
    validation_data = (val_X, val_y),
    epochs = EPOCHS, batch_size=64,
    shuffle = True,
    verbose = 1,
    callbacks = callbacks_list
)

pd.DataFrame(historyCNN.history).to_csv('historyCNN.csv')
modelCNN.save('last_modelCNN.h5')

train_loss = historyCNN.history['loss']
val_loss = historyCNN.history['val_loss']

train_acc = historyCNN.history['acc']
val_acc = historyCNN.history['val_acc']

lr = historyCNN.history['lr']

x = np.arange(1, EPOCHS+1)

#compares training and validation loss across all epochs
plt.plot(x, train_loss, 'b--', label='Training loss')
plt.plot(x, val_loss, 'r-', label='Validation loss')
plt.title('Training and Validation loss')
plt.legend()
plt.show()

#compares training and validation accuracy across all epochs
plt.plot(x, train_acc, 'b--', label='Training accuracy')
plt.plot(x, val_acc, 'r-', label='Validation accuracy')
plt.title('Training and Validation accuracy')
plt.legend()
plt.show()

#plots the learning rate showing change after 8th and 12th epochs
plt.plot(x, lr, 'g-', label='Learning Rate')
plt.yscale('log')
plt.title('Learning rate')
plt.legend()
plt.show()

modelCNN.load_weights('last_modelCNN.h5')
modelCNN.compile(        #last model
    optimizer='rmsprop',
    loss='sparse_categorical_crossentropy',
    metrics=['acc']
)
print("\t\tLAST MODEL:")
print('Training set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelCNN.evaluate(train_X, train_y, verbose=0)))
print('Validation set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelCNN.evaluate(val_X, val_y, verbose=0)))
print('Testing set metrics:\t\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelCNN.evaluate(test_X, test_y, verbose=0)))

best_modelCNN = models.load_model('best_modelCNN.h5')
best_modelCNN.compile(
    optimizer='rmsprop',
    loss='sparse_categorical_crossentropy',
    metrics=['acc']
)
print("\n\t\tBEST MODEL:")
print('Training set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(best_modelCNN.evaluate(train_X, train_y, verbose=0)))
print('Validation set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(best_modelCNN.evaluate(val_X, val_y, verbose=0)))
print('Testing set metrics:\t\tLoss: %.4f\tAccuracy: %.4f' % tuple(best_modelCNN.evaluate(test_X, test_y, verbose=0)))

from sklearn.metrics import precision_score, recall_score, f1_score

# Assuming predictions are obtained using modelCNN.predict()

predictions = modelCNN.predict(test_X)
predicted_labels = np.argmax(predictions, axis=1)

precision = precision_score(test_y, predicted_labels, average='weighted')
recall = recall_score(test_y, predicted_labels, average='weighted')
f1 = f1_score(test_y, predicted_labels, average='weighted')

print('Precision: %.4f' % precision)
print('Recall: %.4f' % recall)
print('F1 Score: %.4f' % f1)

"""#Bidirectional GRU"""

modelGRU = models.Sequential(layers=[
    layers.Embedding(input_dim=max_words, output_dim=embed_dim, input_length=maxlen),
    layers.Bidirectional(layers.GRU(32, activation='relu', return_sequences=True, dropout=.1, recurrent_dropout=.1)),
    layers.Bidirectional(layers.GRU(32, activation='relu', return_sequences=False, dropout=.1, recurrent_dropout=.1)),
    layers.Dense(64, activation='relu', kernel_regularizer='l2'),
    layers.BatchNormalization(),
    layers.Dropout(.2),
    layers.Dense(32, activation='relu', kernel_regularizer='l2'),
    layers.BatchNormalization(),
    layers.Dropout(.1),
    layers.Dense(15, activation='softmax')      #number of classes
])
#modelGRU.summary()

def learningRate_scheduler(epoch, lr):
    if epoch==8 or epoch==12:
        return lr/8
    else:
        return lr


callbacks_list = [
    callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, save_freq='epoch'),
    callbacks.LearningRateScheduler(learningRate_scheduler)
]

modelGRU.compile(
    optimizer='rmsprop',
    loss='sparse_categorical_crossentropy',
    metrics=['acc']
)

EPOCHS = 16

historyGRU = modelGRU.fit(
    train_X, train_y,
    validation_data = (val_X, val_y),
    epochs = EPOCHS, batch_size=64,
    shuffle = True,
    verbose = 1,
    callbacks = callbacks_list
)

pd.DataFrame(historyGRU.history).to_csv('historyGRU.csv')
modelGRU.save('last_modelGRU.h5')

train_loss = historyGRU.history['loss']
val_loss = historyGRU.history['val_loss']

train_acc = historyGRU.history['acc']
val_acc = historyGRU.history['val_acc']

lr = historyGRU.history['lr']

x = np.arange(1, EPOCHS+1)

#compares training and validation loss across all epochs
plt.plot(x, train_loss, 'b--', label='Training loss')
plt.plot(x, val_loss, 'r-', label='Validation loss')
plt.title('Training and Validation loss')
plt.legend()
plt.show()

#compares training and validation accuracy across all epochs
plt.plot(x, train_acc, 'b--', label='Training accuracy')
plt.plot(x, val_acc, 'r-', label='Validation accuracy')
plt.title('Training and Validation accuracy')
plt.legend()
plt.show()

#plots the learning rate showing change after 8th and 12th epochs
plt.plot(x, lr, 'g-', label='Learning Rate')
plt.yscale('log')
plt.title('Learning rate')
plt.legend()
plt.show()

modelGRU.load_weights('last_modelGRU.h5')
modelGRU.compile(        #last model
    optimizer='rmsprop',
    loss='sparse_categorical_crossentropy',
    metrics=['acc']
)
print("\t\tLAST MODEL:")
print('Training set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelGRU.evaluate(train_X, train_y, verbose=0)))
print('Validation set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelGRU.evaluate(val_X, val_y, verbose=0)))
print('Testing set metrics:\t\tLoss: %.4f\tAccuracy: %.4f' % tuple(modelGRU.evaluate(test_X, test_y, verbose=0)))

best_model = models.load_model('best_model.h5')
best_model.compile(
    optimizer='rmsprop',
    loss='sparse_categorical_crossentropy',
    metrics=['acc']
)
print("\n\t\tBEST MODEL:")
print('Training set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(best_model.evaluate(train_X, train_y, verbose=0)))
print('Validation set metrics:\tLoss: %.4f\tAccuracy: %.4f' % tuple(best_model.evaluate(val_X, val_y, verbose=0)))
print('Testing set metrics:\t\tLoss: %.4f\tAccuracy: %.4f' % tuple(best_model.evaluate(test_X, test_y, verbose=0)))

from sklearn.metrics import precision_score, recall_score, f1_score

# Assuming predictions are obtained using modelCNN.predict()

predictions = modelGRU.predict(test_X)
predicted_labels = np.argmax(predictions, axis=1)

precision = precision_score(test_y, predicted_labels, average='weighted')
recall = recall_score(test_y, predicted_labels, average='weighted')
f1 = f1_score(test_y, predicted_labels, average='weighted')

print('Precision: %.4f' % precision)
print('Recall: %.4f' % recall)
print('F1 Score: %.4f' % f1)

cm = np.zeros(shape=(15,15))

for X, y in tqdm(zip(test_X, test_y)):
    pred = modelGRU.predict(X.reshape(1,-1)).argmax().astype(np.int32)
    y = int(y)
    cm[pred,y] += 1

cm = pd.DataFrame(cm, columns=senders_list, index=senders_list)
cm

"""Heatmap of confusion matrix"""

# Getting sender names
sender_names = senders_list

# Normalize the confusion matrix
cm_normalized = cm.astype('float') / cm.sum(axis=0)  # Note the axis change here

# Plot the heatmap with sender names on both axes
plt.figure(figsize=(10, 8))
sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt=".2f", xticklabels=sender_names, yticklabels=sender_names[::1])
plt.xlabel('Predicted Label')
plt.ylabel('Actual Label')
plt.title('Normalized Confusion Matrix')
plt.show()

"""Lets see which senders are most often confused"""

cl = []
for row in cm.index:
    for col in cm.columns:
        cl.append([cm.loc[col, row], row, col])

cl = pd.DataFrame(cl, columns=['Confusions', 'Predicted', 'Actual']).sort_values(by='Confusions', ascending=False)
cl = cl.query('`Predicted`!=`Actual`').reset_index(drop=True)
cl.head()

"""checking if these relations are symmetric"""

cl['Reverse'] = cl.apply(lambda x: cl.query('`Predicted`==@x.Actual & `Actual`==@x.Predicted').values[0][0] , axis=1)
cl.iloc[:15]